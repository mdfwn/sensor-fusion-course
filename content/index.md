# Interactive Sensor Fusion Course

Welcome to the **Interactive Python-First Sensor Fusion Course** — a comprehensive, hands-on program designed to teach you the fundamentals of sensor fusion and perception for autonomous vehicles.

```{admonition} 🎯 What You'll Build
:class: tip
By the end of this course, you'll have built a complete sensor fusion pipeline that combines lidar, radar, and camera data to detect, classify, and track objects in real-time for autonomous vehicles.
```

## 🌟 Course Overview

This nanodegree-style program transforms traditional sensor fusion education with:

- **Interactive Python Notebooks** — Write and execute code directly in your browser
- **Real-World Datasets** — Work with actual autonomous vehicle sensor data
- **Progressive Learning** — Build skills incrementally across 4 specialized courses
- **Hands-On Projects** — Apply theory immediately with practical exercises
- **Modern Tools** — Learn with NumPy, OpenCV, Open3D, and other industry-standard libraries

## 📚 Course Structure

### 🔍 **Course 1: Lidar Obstacle Detection**
Learn to process 3D point clouds, implement RANSAC for plane fitting, and perform object clustering with KD-trees.

**Key Skills:** Point cloud processing, RANSAC algorithm, Euclidean clustering, bounding box generation

### 📷 **Course 2: Camera Tracking & Detection**
Master computer vision techniques for feature detection, object tracking, and collision avoidance.

**Key Skills:** Camera calibration, feature descriptors (SIFT/SURF/ORB), YOLO object detection, multi-object tracking

### 📡 **Course 3: Radar Clustering & Tracking**
Understand radar principles, implement CFAR detection, and track multiple targets with advanced algorithms.

**Key Skills:** FFT-based processing, CFAR thresholding, angle estimation, data association

### 🧮 **Course 4: Kalman Filters & Multi-Sensor Fusion**
Implement advanced filtering techniques and fuse data from multiple sensors for robust tracking.

**Key Skills:** Extended/Unscented Kalman Filters, coordinate transforms, track-to-track fusion

### 🎓 **Capstone Project: End-to-End Pipeline**
Integrate all techniques into a complete autonomous vehicle perception system.

## ⏱️ Time Commitment

- **Total Duration:** Approximately 63 hours
- **Flexible Schedule:** Learn at your own pace
- **Estimated Completion:** 6-8 weeks (8-10 hours per week)

## 📋 Prerequisites

Before starting this course, you should have:

### 🐍 **Programming**
- **Intermediate Python** — Functions, classes, data structures
- **Basic NumPy/SciPy** — Array operations, mathematical functions
- **Git/GitHub** — Version control basics

### 📐 **Mathematics**
- **Linear Algebra** — Vectors, matrices, transformations
- **Probability & Statistics** — Distributions, Bayes' theorem
- **Basic Calculus** — Derivatives, optimization concepts

### 🔧 **Technical**
- **Linux Command Line** — Navigation, file operations
- **Basic Physics** — Kinematics, wave properties
- **Jupyter Notebooks** — Interactive computing environment

```{admonition} 💡 New to These Topics?
:class: note
Don't worry if you need to brush up! Check out our [Resources page](resources.md) for recommended tutorials and refresher materials.
```

## 🌟 Learning Outcomes

Upon completing this course, you will be able to:

1. **Design and implement** lidar segmentation and clustering algorithms
2. **Develop** camera-based object tracking systems with modern feature descriptors
3. **Build** radar processing pipelines with CFAR detection and angle estimation
4. **Create** multi-sensor Kalman filter systems for robust object tracking
5. **Deploy** a real-time sensor fusion perception stack for autonomous vehicles
6. **Evaluate** system performance using industry-standard metrics (precision, recall, RMSE)

## 🚀 Why Python?

While traditional sensor fusion often uses C++ for real-time performance, this course leverages Python's ecosystem for rapid prototyping and learning:

- **NumPy/SciPy** — Vectorized operations approaching native speed
- **Open3D** — Advanced 3D processing and visualization
- **OpenCV** — Industry-standard computer vision library
- **FilterPy** — Sophisticated Kalman filtering implementations
- **Plotly** — Interactive 3D visualizations and data exploration

```{admonition} 🎯 Ready to Start?
:class: tip
Head over to [How to Use This Course](how_to_use.md) to learn about the interactive features, or jump straight into [Course 1: Lidar](lidar/index.md) to begin your sensor fusion journey!
```

---

## 👥 Meet Your Instructors

### David Silver
**Senior Curriculum Lead**  
Pioneer in autonomous vehicle education with extensive experience at Udacity and the self-driving car industry.

### Dr. Andreas Haja
**Autonomous Driving Researcher & Educator**  
PhD in sensor fusion with publications in leading robotics conferences and practical industry experience.

### Stephen Welch
**Computer Vision Engineer & Instructor**  
Expert in real-time computer vision systems with a passion for making complex topics accessible.

---

## 📞 Support & Community

- 🤝 **Mentor Network** — Get technical questions answered by industry experts
- 📝 **Project Reviews** — Receive personalized feedback on your implementations
- 💼 **Career Services** — Resume review, LinkedIn optimization, GitHub portfolio tips
- 💬 **Community Forum** — Connect with fellow students and collaborate on projects

---

*Let's build the future of autonomous vehicles together! 🚗💨*