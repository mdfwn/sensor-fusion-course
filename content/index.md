# Interactive Sensor Fusion Course

Welcome to the **Interactive Python-First Sensor Fusion Course** â€” a comprehensive, hands-on program designed to teach you the fundamentals of sensor fusion and perception for autonomous vehicles.

```{admonition} ğŸ¯ What You'll Build
:class: tip
By the end of this course, you'll have built a complete sensor fusion pipeline that combines lidar, radar, and camera data to detect, classify, and track objects in real-time for autonomous vehicles.
```

## ğŸŒŸ Course Overview

This nanodegree-style program transforms traditional sensor fusion education with:

- **Interactive Python Notebooks** â€” Write and execute code directly in your browser
- **Real-World Datasets** â€” Work with actual autonomous vehicle sensor data
- **Progressive Learning** â€” Build skills incrementally across 4 specialized courses
- **Hands-On Projects** â€” Apply theory immediately with practical exercises
- **Modern Tools** â€” Learn with NumPy, OpenCV, Open3D, and other industry-standard libraries

## ğŸ“š Course Structure

### ğŸ” **Course 1: Lidar Obstacle Detection**
Learn to process 3D point clouds, implement RANSAC for plane fitting, and perform object clustering with KD-trees.

**Key Skills:** Point cloud processing, RANSAC algorithm, Euclidean clustering, bounding box generation

### ğŸ“· **Course 2: Camera Tracking & Detection**
Master computer vision techniques for feature detection, object tracking, and collision avoidance.

**Key Skills:** Camera calibration, feature descriptors (SIFT/SURF/ORB), YOLO object detection, multi-object tracking

### ğŸ“¡ **Course 3: Radar Clustering & Tracking**
Understand radar principles, implement CFAR detection, and track multiple targets with advanced algorithms.

**Key Skills:** FFT-based processing, CFAR thresholding, angle estimation, data association

### ğŸ§® **Course 4: Kalman Filters & Multi-Sensor Fusion**
Implement advanced filtering techniques and fuse data from multiple sensors for robust tracking.

**Key Skills:** Extended/Unscented Kalman Filters, coordinate transforms, track-to-track fusion

### ğŸ“ **Capstone Project: End-to-End Pipeline**
Integrate all techniques into a complete autonomous vehicle perception system.

## â±ï¸ Time Commitment

- **Total Duration:** Approximately 63 hours
- **Flexible Schedule:** Learn at your own pace
- **Estimated Completion:** 6-8 weeks (8-10 hours per week)

## ğŸ“‹ Prerequisites

Before starting this course, you should have:

### ğŸ **Programming**
- **Intermediate Python** â€” Functions, classes, data structures
- **Basic NumPy/SciPy** â€” Array operations, mathematical functions
- **Git/GitHub** â€” Version control basics

### ğŸ“ **Mathematics**
- **Linear Algebra** â€” Vectors, matrices, transformations
- **Probability & Statistics** â€” Distributions, Bayes' theorem
- **Basic Calculus** â€” Derivatives, optimization concepts

### ğŸ”§ **Technical**
- **Linux Command Line** â€” Navigation, file operations
- **Basic Physics** â€” Kinematics, wave properties
- **Jupyter Notebooks** â€” Interactive computing environment

```{admonition} ğŸ’¡ New to These Topics?
:class: note
Don't worry if you need to brush up! Check out our [Resources page](resources.md) for recommended tutorials and refresher materials.
```

## ğŸŒŸ Learning Outcomes

Upon completing this course, you will be able to:

1. **Design and implement** lidar segmentation and clustering algorithms
2. **Develop** camera-based object tracking systems with modern feature descriptors
3. **Build** radar processing pipelines with CFAR detection and angle estimation
4. **Create** multi-sensor Kalman filter systems for robust object tracking
5. **Deploy** a real-time sensor fusion perception stack for autonomous vehicles
6. **Evaluate** system performance using industry-standard metrics (precision, recall, RMSE)

## ğŸš€ Why Python?

While traditional sensor fusion often uses C++ for real-time performance, this course leverages Python's ecosystem for rapid prototyping and learning:

- **NumPy/SciPy** â€” Vectorized operations approaching native speed
- **Open3D** â€” Advanced 3D processing and visualization
- **OpenCV** â€” Industry-standard computer vision library
- **FilterPy** â€” Sophisticated Kalman filtering implementations
- **Plotly** â€” Interactive 3D visualizations and data exploration

```{admonition} ğŸ¯ Ready to Start?
:class: tip
Head over to [How to Use This Course](how_to_use.md) to learn about the interactive features, or jump straight into [Course 1: Lidar](lidar/index.md) to begin your sensor fusion journey!
```

---

## ğŸ‘¥ Meet Your Instructors

### David Silver
**Senior Curriculum Lead**  
Pioneer in autonomous vehicle education with extensive experience at Udacity and the self-driving car industry.

### Dr. Andreas Haja
**Autonomous Driving Researcher & Educator**  
PhD in sensor fusion with publications in leading robotics conferences and practical industry experience.

### Stephen Welch
**Computer Vision Engineer & Instructor**  
Expert in real-time computer vision systems with a passion for making complex topics accessible.

---

## ğŸ“ Support & Community

- ğŸ¤ **Mentor Network** â€” Get technical questions answered by industry experts
- ğŸ“ **Project Reviews** â€” Receive personalized feedback on your implementations
- ğŸ’¼ **Career Services** â€” Resume review, LinkedIn optimization, GitHub portfolio tips
- ğŸ’¬ **Community Forum** â€” Connect with fellow students and collaborate on projects

---

*Let's build the future of autonomous vehicles together! ğŸš—ğŸ’¨*